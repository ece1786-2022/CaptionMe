{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f91fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# csv_path = 'test.csv'\n",
    "\n",
    "# dictionary = {'a':['123','234','345'], 'b':['567','678','789']}\n",
    "# df = pd.DataFrame.from_dict(dictionary, orient='columns')\n",
    "# print(df)\n",
    "# df.to_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7ed2d",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf01e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext\n",
    "import math\n",
    "from PIL import Image\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from mingpt.utils import CfgNode as CN\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac7cf1",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31db5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = './annotations_t.csv'\n",
    "image_path = './archive/train_t'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 8\n",
    "epochs = 110\n",
    "n_embed = 100\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\",dim=n_embed) # embedding size = 100  \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f09c2",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc46f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Train_Dataset(Dataset):\n",
    "    def __init__(self, image_folder_path, vocab, csv_path, transformer):\n",
    "        self.image_folder_path = image_folder_path\n",
    "        annotations = pd.read_csv(csv_path)\n",
    "        self.captions = np.array(annotations['captions'])\n",
    "        self.image_names = np.array(annotations['file_directory'])    \n",
    "        self.transformer = transformer\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_full_name = self.image_folder_path + '/' + self.image_names[idx]\n",
    "        x = Image.open(image_full_name)\n",
    "        if self.transformer is not None:\n",
    "            x = self.transformer(x)\n",
    "#             x = x.permute(1,2,0)\n",
    "        ground_truth_cap = self.captions[idx]\n",
    "        V = len(self.vocab.vectors)\n",
    "        L = ground_truth_cap.split()\n",
    "        tokenized_caption = torch.tensor([self.vocab.stoi.get(w.lower(), V-1) for w in L])  # Use the last word in the vocab as the \"out-of-vocabulary\" token\n",
    "        if V-1 in tokenized_caption:\n",
    "            print('Wrong Labelling')\n",
    "#         print(image_full_name, ground_truth_cap)        \n",
    "        return x, tokenized_caption\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab.vectors)  \n",
    "    \n",
    "    def get_block_size(self):\n",
    "        all_captions_len = []\n",
    "        for i in range(len(self.captions)):\n",
    "            all_captions_len.append(len(self.captions[i].split()))\n",
    "        return max(all_captions_len)+1\n",
    "    \n",
    "custom_transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.3),\n",
    "    torchvision.transforms.RandomRotation(degrees=10),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "def lm_collate_fn(batch, device):\n",
    "    x = [item[0] for item in batch]  \n",
    "    y_input = [item[1][:-1] for item in batch]  \n",
    "    y_label = [item[1] for item in batch]\n",
    "    maxlen_input = max([len(s) for s in y_input])\n",
    "    maxlen_label = max([len(s) for s in y_label])\n",
    "    padding_value = glove.stoi.get('unk')\n",
    "    #x from first word to the second last word, y from second word to the last word\n",
    "    input_cap, label_cap = [], []\n",
    "    for sy_i, sy_l in zip(y_input, y_label):\n",
    "        input_cap.append(torch.cat([sy_i, torch.ones(maxlen_input - len(sy_i))*padding_value]))\n",
    "        label_cap.append(torch.cat([sy_l, torch.ones(maxlen_label - len(sy_l))*padding_value]))\n",
    "    return torch.stack(x).long().to(device), torch.stack(input_cap).long().to(device), torch.stack(label_cap).long().to(device)\n",
    "  \n",
    "train_dataset = Custom_Train_Dataset(image_path, glove, csv_path, custom_transformer)\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn = lambda batch: lm_collate_fn(batch, device))\n",
    "\n",
    "vocab_size = train_dataset.get_vocab_size()\n",
    "block_size = train_dataset.get_block_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b624598",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018500df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x.shape = (batch, sent len, embedding)\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\" GPT Language Model \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
    "        C.model_type = 'gpt'\n",
    "        C.n_layer = None\n",
    "        C.n_head = None\n",
    "        C.n_embd =  None\n",
    "        # these options must be filled in externally\n",
    "        C.vocab_size = None\n",
    "        C.block_size = None\n",
    "        # dropout hyperparameters\n",
    "        C.embd_pdrop = 0.1\n",
    "        C.resid_pdrop = 0.1\n",
    "        C.attn_pdrop = 0.1\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, vocab):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        type_given = config.model_type is not None\n",
    "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
    "        assert type_given ^ params_given # exactly one of these (XOR)\n",
    "        if type_given:\n",
    "            # translate from model_type to detailed configuration\n",
    "            config.merge_from_dict({\n",
    "                # names follow the huggingface naming conventions\n",
    "                # GPT-1\n",
    "                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n",
    "                # GPT-2 configs\n",
    "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "                # Gophers\n",
    "                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
    "                # (there are a number more...)\n",
    "                # I made these tiny models up\n",
    "                'gpt-mini':     dict(n_layer=5, n_head=5, n_embd=n_embed),\n",
    "                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
    "                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
    "            }[config.model_type])\n",
    "\n",
    "        #wte is embedding for words\n",
    "        #wpe is embedding for positions\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding.from_pretrained(vocab.vectors, freeze = True),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config, cnn_model_params):\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "        if cnn_model_params is not None:\n",
    "            # create the pytorch optimizer object\n",
    "            optim_groups = [\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "                {\"params\": cnn_model_params,'lr': 1e-5}\n",
    "            ]\n",
    "            \n",
    "            n_parameters_transformer = sum(p.numel()\n",
    "               for p in self.parameters() if p.requires_grad)\n",
    "            n_parameters_cnn = sum(p.numel()\n",
    "                           for p in cnn_model_params if p.requires_grad)\n",
    "            print(f\"Number of trainable params: {n_parameters_transformer + n_parameters_cnn}\")\n",
    "        else:\n",
    "            # create the pytorch optimizer object\n",
    "            optim_groups = [\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0}\n",
    "            ]  \n",
    "        \n",
    "            n_parameters_transformer = sum(p.numel()\n",
    "                           for p in self.parameters() if p.requires_grad)\n",
    "            print(f\"Number of trainable params: {n_parameters_transformer}\")\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, image_embed, idx=None, targets=None, finetune_classify=False):\n",
    "\n",
    "        device = image_embed.device\n",
    "  \n",
    "        if idx is not None:\n",
    "            b, t = idx.size()\n",
    "            assert t <= self.block_size, f\"Cannot forwarnd sequence of length {t}, block size is only {self.block_size}\"\n",
    "            pos = torch.arange(0, t+1, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "            # forward the GPT model itself\n",
    "            tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            tok_emb = torch.cat((image_embed.unsqueeze(1), tok_emb), 1)\n",
    "          \n",
    "        else:\n",
    "            pos = torch.arange(0, 1, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "            tok_emb = image_embed.unsqueeze(1)   \n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        \n",
    "        assert tok_emb[0].shape == pos_emb[0].shape, f\"wrong token or position embedding\"\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        #x.shape = (batch, sentence len, embedding)\n",
    "        if not finetune_classify:\n",
    "            # LM forward procedure\n",
    "            logits = self.lm_head(x)\n",
    "        else:\n",
    "            # Finetune classify procedure\n",
    "            print('error')\n",
    "            return\n",
    "            \n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbf39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 40.61M\n"
     ]
    }
   ],
   "source": [
    "# set up model configurations\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-mini'\n",
    "model_config.vocab_size = vocab_size\n",
    "#block_size is a max sentence length in dataset\n",
    "model_config.block_size = block_size\n",
    "model = GPT(model_config, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25c008",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        # device to train on\n",
    "        C.device = 'auto'\n",
    "        # dataloder parameters\n",
    "        C.num_workers = 4\n",
    "        # optimizer parameters\n",
    "        C.max_iters = None\n",
    "        C.batch_size = 64\n",
    "        C.learning_rate = 3e-4\n",
    "        C.betas = (0.9, 0.95)\n",
    "        C.weight_decay = 0.1 # only applied on matmul weights\n",
    "        C.grad_norm_clip = 1.0\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, model, train_loader, epochs, downstream_finetune = False):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_loader = train_loader\n",
    "        self.epochs = epochs\n",
    "        self.callbacks = defaultdict(list)\n",
    "        self.downstream_finetune = False\n",
    "        # determine the device we'll train on\n",
    "        if config.device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = config.device\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(\"running on device\", self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "    def add_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "\n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "        cnn_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True,)\n",
    "        for param in cnn_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        cnn_model.classifier = nn.Sequential(nn.Linear(1280, 512),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(0.2),\n",
    "                                            nn.Linear(512, n_embed))\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        # setup the optimizer\n",
    "        self.optimizer = model.configure_optimizers(config, cnn_model.parameters())\n",
    "        train_loss_set = []\n",
    "        # setup the dataloader     \n",
    "        for epoch in range(self.epochs):\n",
    "            print('Epoch ' + str(epoch) + ':')\n",
    "            model.train()\n",
    "            cnn_model.train()\n",
    "            train_loss_epoch = []\n",
    "            \n",
    "            #train model and calculate training accuracy\n",
    "            for x, y_input, y_label in train_loader:\n",
    "                # forward the model\n",
    "                x = x.type(torch.FloatTensor).to(device)\n",
    "                image_embed = cnn_model(x)\n",
    "                logits, self.loss = model(image_embed.to(self.device), y_input, y_label, self.downstream_finetune)\n",
    "                train_loss_epoch.append(self.loss.detach().cpu().item())\n",
    "                \n",
    "                # backprop and update the parameters\n",
    "                model.zero_grad(set_to_none=True)\n",
    "                self.loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                self.optimizer.step()\n",
    "                del x, y_input, y_label\n",
    "\n",
    "            print('Training Loss is : {}'.format(sum(train_loss_epoch)/len(train_loss_epoch)))\n",
    "            train_loss_set.append(sum(train_loss_epoch)/len(train_loss_epoch))\n",
    "        \n",
    "        torch.save(model.state_dict(), 'transformer.pt')\n",
    "        torch.save(cnn_model.state_dict(), 'cnn.pt')\n",
    "        \n",
    "        return train_loss_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e29acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start training\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 8e-5 # the model we're using is so small that we can go a bit faster\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_loader, epochs, downstream_finetune = False)\n",
    "\n",
    "# Train!\n",
    "train_loss_set = trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50483b61",
   "metadata": {},
   "source": [
    "# Load Model and Generate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df6085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zixua/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\zixua\\anaconda3\\envs\\test0\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\zixua\\anaconda3\\envs\\test0\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "cnn_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True,)\n",
    "cnn_model.classifier = nn.Sequential(nn.Linear(1280, 512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(512, n_embed))\n",
    "\n",
    "model.load_state_dict(torch.load('transformer.pt'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "cnn_model.load_state_dict(torch.load('cnn.pt'))\n",
    "cnn_model.eval()\n",
    "cnn_model.to(device)\n",
    "max_new_tokens = block_size-1\n",
    "image_path = 'test_image_2.jpg'\n",
    "\n",
    "valid_transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30d4a582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7190, 0.2810]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "he i\n",
      "tensor([[1., 0.]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "is the\n",
      "tensor([[1., 0.]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "a the\n",
      "tensor([[1., 0.]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "dog the\n",
      "tensor([[1., 0.]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      ". the\n",
      "['he', 'is', 'a', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "def generate(image_path, device, max_new_tokens, temperature=1.0, top_p=None, top_k=None):\n",
    "    if top_p is not None and top_k is not None:\n",
    "        print('Only one sampling approach is allowed')\n",
    "        return\n",
    "    if top_p is None and top_k is None:\n",
    "        print('You must select a sampling approach')\n",
    "        return\n",
    "    \n",
    "    generation_prob = []\n",
    "    generation_word = []\n",
    "    idx = None\n",
    "    image_file = Image.open(image_path)\n",
    "    image_tensor = valid_transformer(image_file).unsqueeze(0).to(device)\n",
    "    image_embedding = cnn_model(image_tensor)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(image_embedding, idx)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        if top_k is not None:\n",
    "            idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "            \n",
    "        if top_p is not None:\n",
    "            top_k = 1\n",
    "            idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "            while idx_next_prob.detach().cpu().sum().item() < top_p:\n",
    "                top_k += 1\n",
    "                idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "                \n",
    "        idx_next_prob, idx_next = idx_next_prob[0].detach().cpu(), idx_next[0].detach().cpu()\n",
    "        sum_prob = idx_next_prob.sum().item()\n",
    "        idx_next_prob = [prob_i/sum_prob for prob_i in idx_next_prob.tolist()]\n",
    "        idx_next_prob[-1] = 1 - sum(idx_next_prob[:-1])\n",
    "\n",
    "        select_token = np.random.choice(idx_next.tolist(), 1, p=idx_next_prob)  \n",
    "        for token, token_prob in zip(idx_next, idx_next_prob):\n",
    "            if token == select_token[0]:\n",
    "                generation_prob.append(token_prob)    \n",
    "\n",
    "        generation_word.append(glove.itos[select_token[0]])\n",
    "        test_prob, test_next = torch.topk(probs, k=2, dim=-1)\n",
    "        print(test_prob)\n",
    "        print(glove.itos[test_next[0, 0].detach().cpu().item()], glove.itos[test_next[0, 1].detach().cpu().item()])            \n",
    "            \n",
    "            \n",
    "        idx_next = torch.tensor(select_token).unsqueeze(0).to(device)\n",
    "        if idx is None:\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = idx_next\n",
    "        else:\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        if generation_word[-1] == '.':\n",
    "            break\n",
    "\n",
    "    return generation_word, generation_prob\n",
    "\n",
    "generation_word, generation_prob = generate(image_path, device, max_new_tokens, temperature=0.01, top_p=None, top_k=1)\n",
    "print(generation_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46efecaf",
   "metadata": {},
   "source": [
    "# Compute Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b7339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [\n",
    "    'he is a not dog .'.split(),\n",
    "    'he is a good dog .'.split(),\n",
    "    'he is the best dog .'.split()\n",
    "]\n",
    "print(reference)\n",
    "print('BLEU score -> {}'.format(sentence_bleu(reference, generation_word, weights=(0.7,0.2,0.1))))\n",
    "#https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dea6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574dc33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
