{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f91fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# csv_path = 'test.csv'\n",
    "\n",
    "# dictionary = {'a':['123','234','345'], 'b':['567','678','789']}\n",
    "# df = pd.DataFrame.from_dict(dictionary, orient='columns')\n",
    "# print(df)\n",
    "# df.to_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7ed2d",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf01e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext\n",
    "import math\n",
    "from PIL import Image\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from mingpt.utils import CfgNode as CN\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac7cf1",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31db5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../All_Data/annotations_train.csv'\n",
    "image_path = '../All_Data/train'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "n_embed = 100\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\",dim=n_embed) # embedding size = 100  \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f09c2",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc46f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Train_Dataset(Dataset):\n",
    "    def __init__(self, image_folder_path, vocab, csv_path, transformer):\n",
    "        self.image_folder_path = image_folder_path\n",
    "        annotations = pd.read_csv(csv_path)\n",
    "        self.captions = np.array(annotations['captions'])\n",
    "        self.image_names = np.array(annotations['file_directory'])    \n",
    "        self.transformer = transformer\n",
    "        self.replace_dict = {'Cockapoo':'parrot', 'Dalmation':'Dalmatian', 'Bluetick':'Bluebonnet'}\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_full_name = self.image_folder_path + '/' + self.image_names[idx]\n",
    "        x = Image.open(image_full_name)\n",
    "        if self.transformer is not None:\n",
    "            x = self.transformer(x)\n",
    "#             x = x.permute(1,2,0)\n",
    "        ground_truth_cap = self.captions[idx]\n",
    "        V = len(self.vocab.vectors)\n",
    "        L = ground_truth_cap.split()\n",
    "        for i, word in enumerate (L):\n",
    "            if word in self.replace_dict.keys():\n",
    "                L[i] = self.replace_dict[word]\n",
    "                \n",
    "        tokenized_caption = torch.tensor([self.vocab.stoi.get(w.lower(), V-1) for w in L])  # Use the last word in the vocab as the \"out-of-vocabulary\" token\n",
    "        if V-1 in tokenized_caption:\n",
    "            print('Wrong Labelling')\n",
    "#         print(image_full_name, ground_truth_cap)        \n",
    "        return x, tokenized_caption\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab.vectors)  \n",
    "    \n",
    "    def get_block_size(self):\n",
    "        all_captions_len = []\n",
    "        for i in range(len(self.captions)):\n",
    "            all_captions_len.append(len(self.captions[i].split()))\n",
    "        return max(all_captions_len)+1\n",
    "    \n",
    "custom_transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.3),\n",
    "    torchvision.transforms.RandomRotation(degrees=10),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "def lm_collate_fn(batch, device):\n",
    "    x = [item[0] for item in batch]  \n",
    "    y_input = [item[1][:-1] for item in batch]  \n",
    "    y_label = [item[1] for item in batch]\n",
    "    maxlen_input = max([len(s) for s in y_input])\n",
    "    maxlen_label = max([len(s) for s in y_label])\n",
    "    padding_value = glove.stoi.get('unk')\n",
    "    #x from first word to the second last word, y from second word to the last word\n",
    "    input_cap, label_cap = [], []\n",
    "    for sy_i, sy_l in zip(y_input, y_label):\n",
    "        input_cap.append(torch.cat([sy_i, torch.ones(maxlen_input - len(sy_i))*padding_value]))\n",
    "        label_cap.append(torch.cat([sy_l, torch.ones(maxlen_label - len(sy_l))*padding_value]))\n",
    "    return torch.stack(x).long().to(device), torch.stack(input_cap).long().to(device), torch.stack(label_cap).long().to(device)\n",
    "  \n",
    "train_dataset = Custom_Train_Dataset(image_path, glove, csv_path, custom_transformer)\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn = lambda batch: lm_collate_fn(batch, device))\n",
    "\n",
    "vocab_size = train_dataset.get_vocab_size()\n",
    "block_size = train_dataset.get_block_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b624598",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018500df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x.shape = (batch, sent len, embedding)\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\" GPT Language Model \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
    "        C.model_type = 'gpt'\n",
    "        C.n_layer = None\n",
    "        C.n_head = None\n",
    "        C.n_embd =  None\n",
    "        # these options must be filled in externally\n",
    "        C.vocab_size = None\n",
    "        C.block_size = None\n",
    "        # dropout hyperparameters\n",
    "        C.embd_pdrop = 0.1\n",
    "        C.resid_pdrop = 0.1\n",
    "        C.attn_pdrop = 0.1\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, vocab):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        type_given = config.model_type is not None\n",
    "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
    "        assert type_given ^ params_given # exactly one of these (XOR)\n",
    "        if type_given:\n",
    "            # translate from model_type to detailed configuration\n",
    "            config.merge_from_dict({\n",
    "                # names follow the huggingface naming conventions\n",
    "                # GPT-1\n",
    "                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n",
    "                # GPT-2 configs\n",
    "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "                # Gophers\n",
    "                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
    "                # (there are a number more...)\n",
    "                # I made these tiny models up\n",
    "                'gpt-mini':     dict(n_layer=5, n_head=5, n_embd=n_embed),\n",
    "                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
    "                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
    "            }[config.model_type])\n",
    "\n",
    "        #wte is embedding for words\n",
    "        #wpe is embedding for positions\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding.from_pretrained(vocab.vectors, freeze = True),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config, cnn_model_params):\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "        if cnn_model_params is not None:\n",
    "            # create the pytorch optimizer object\n",
    "            optim_groups = [\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "                {\"params\": cnn_model_params,'lr': 3e-5}\n",
    "            ]\n",
    "            \n",
    "            n_parameters_transformer = sum(p.numel()\n",
    "               for p in self.parameters() if p.requires_grad)\n",
    "            n_parameters_cnn = sum(p.numel()\n",
    "                           for p in cnn_model_params if p.requires_grad)\n",
    "            print(f\"Number of trainable params: {n_parameters_transformer + n_parameters_cnn}\")\n",
    "        else:\n",
    "            # create the pytorch optimizer object\n",
    "            optim_groups = [\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0}\n",
    "            ]  \n",
    "        \n",
    "            n_parameters_transformer = sum(p.numel()\n",
    "                           for p in self.parameters() if p.requires_grad)\n",
    "            print(f\"Number of trainable params: {n_parameters_transformer}\")\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, image_embed, idx=None, targets=None, finetune_classify=False):\n",
    "\n",
    "        device = image_embed.device\n",
    "  \n",
    "        if idx is not None:\n",
    "            b, t = idx.size()\n",
    "            assert t <= self.block_size, f\"Cannot forwarnd sequence of length {t}, block size is only {self.block_size}\"\n",
    "            pos = torch.arange(0, t+1, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "            # forward the GPT model itself\n",
    "            tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            tok_emb = torch.cat((image_embed.unsqueeze(1), tok_emb), 1)\n",
    "          \n",
    "        else:\n",
    "            pos = torch.arange(0, 1, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "            tok_emb = image_embed.unsqueeze(1)   \n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        \n",
    "        assert tok_emb[0].shape == pos_emb[0].shape, f\"wrong token or position embedding\"\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        #x.shape = (batch, sentence len, embedding)\n",
    "        if not finetune_classify:\n",
    "            # LM forward procedure\n",
    "            logits = self.lm_head(x)\n",
    "        else:\n",
    "            # Finetune classify procedure\n",
    "            print('error')\n",
    "            return\n",
    "            \n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbf39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 40.61M\n"
     ]
    }
   ],
   "source": [
    "# set up model configurations\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-mini'\n",
    "model_config.vocab_size = vocab_size\n",
    "#block_size is a max sentence length in dataset\n",
    "model_config.block_size = block_size\n",
    "model = GPT(model_config, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25c008",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca9c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        # device to train on\n",
    "        C.device = 'auto'\n",
    "        # dataloder parameters\n",
    "        C.num_workers = 4\n",
    "        # optimizer parameters\n",
    "        C.max_iters = None\n",
    "        C.batch_size = 64\n",
    "        C.learning_rate = 3e-4\n",
    "        C.betas = (0.9, 0.95)\n",
    "        C.weight_decay = 0.1 # only applied on matmul weights\n",
    "        C.grad_norm_clip = 1.0\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, model, train_loader, epochs, downstream_finetune = False):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_loader = train_loader\n",
    "        self.epochs = epochs\n",
    "        self.callbacks = defaultdict(list)\n",
    "        self.downstream_finetune = False\n",
    "        # determine the device we'll train on\n",
    "        if config.device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = config.device\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(\"running on device\", self.device)\n",
    "\n",
    "        # variables that will be assigned to trainer class later for logging and etc\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "\n",
    "    def add_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent].append(callback)\n",
    "\n",
    "    def set_callback(self, onevent: str, callback):\n",
    "        self.callbacks[onevent] = [callback]\n",
    "\n",
    "    def trigger_callbacks(self, onevent: str):\n",
    "        for callback in self.callbacks.get(onevent, []):\n",
    "            callback(self)\n",
    "\n",
    "    def run(self):\n",
    "        model, config = self.model, self.config\n",
    "        cnn_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True,)\n",
    "        for param in cnn_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        cnn_model.classifier = nn.Sequential(nn.Linear(1280, 512),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Dropout(0.2),\n",
    "                                            nn.Linear(512, n_embed))\n",
    "        cnn_model = cnn_model.to(device)\n",
    "        # setup the optimizer\n",
    "        self.optimizer = model.configure_optimizers(config, cnn_model.parameters())\n",
    "        train_loss_set = []\n",
    "        # setup the dataloader     \n",
    "        for epoch in range(self.epochs):\n",
    "            print('Epoch ' + str(epoch) + ':')\n",
    "            model.train()\n",
    "            cnn_model.train()\n",
    "            train_loss_epoch = []\n",
    "            \n",
    "            with tqdm.tqdm(total=len(train_loader)) as pbar:\n",
    "                #train model and calculate training accuracy\n",
    "                for x, y_input, y_label in train_loader:\n",
    "                    # forward the model\n",
    "                    x = x.type(torch.FloatTensor).to(device)\n",
    "                    image_embed = cnn_model(x)\n",
    "                    logits, self.loss = model(image_embed.to(self.device), y_input, y_label, self.downstream_finetune)\n",
    "                    train_loss_epoch.append(self.loss.detach().cpu().item())\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad(set_to_none=True)\n",
    "                    self.loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    self.optimizer.step()\n",
    "                    pbar.update(1)\n",
    "                    del x, y_input, y_label\n",
    "\n",
    "                print('Training Loss is : {}'.format(sum(train_loss_epoch)/len(train_loss_epoch)))\n",
    "                train_loss_set.append(sum(train_loss_epoch)/len(train_loss_epoch))\n",
    "                \n",
    "            if epoch > 5 and epoch % 5 == 0:\n",
    "                torch.save(model.state_dict(), './saved_checkpoints/transformer' + str(epoch) +'.pt')\n",
    "                torch.save(cnn_model.state_dict(), './saved_checkpoints/cnn' + str(epoch) +'.pt')\n",
    "        \n",
    "        return train_loss_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e29acb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zixua/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable params: 41315772\n",
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:53<00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 5.370313386122386\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 1.0377190907163478\n",
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:49<00:00, 18.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.7973622804994036\n",
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.73157719704629\n",
      "Epoch 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.696103968153334\n",
      "Epoch 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.674057512905947\n",
      "Epoch 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6624554391720998\n",
      "Epoch 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6544861924970985\n",
      "Epoch 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6448364139805427\n",
      "Epoch 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.635689379508961\n",
      "Epoch 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6334319020880509\n",
      "Epoch 11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6280795997938027\n",
      "Epoch 12:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:49<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6201147165489551\n",
      "Epoch 13:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.620418306685304\n",
      "Epoch 14:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:49<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.623552943079588\n",
      "Epoch 15:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6154320248246447\n",
      "Epoch 16:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6148322306557572\n",
      "Epoch 17:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6109300602361908\n",
      "Epoch 18:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6080049184287429\n",
      "Epoch 19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:50<00:00, 18.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6103356671371277\n",
      "Epoch 20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 942/942 [00:51<00:00, 18.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is : 0.6049496922839726\n",
      "Epoch 21:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▋                                                                         | 90/942 [00:04<00:46, 18.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(train_config, model, train_loader, epochs, downstream_finetune \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train!\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m train_loss_set \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m train_loss_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m#train model and calculate training accuracy\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, y_input, y_label \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[0;32m     74\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     75\u001b[0m         image_embed \u001b[38;5;241m=\u001b[39m cnn_model(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mCustom_Train_Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     16\u001b[0m     image_full_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_folder_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_names[idx]\n\u001b[1;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_full_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\PIL\\Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2950\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   2952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 2953\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2954\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#start training\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 1e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_loader, epochs, downstream_finetune = False)\n",
    "\n",
    "# Train!\n",
    "train_loss_set = trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50483b61",
   "metadata": {},
   "source": [
    "# Load Model and Generate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5df6085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zixua/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "cnn_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True,)\n",
    "cnn_model.classifier = nn.Sequential(nn.Linear(1280, 512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(512, n_embed))\n",
    "\n",
    "model.load_state_dict(torch.load('transformer15.pt'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "cnn_model.load_state_dict(torch.load('cnn15.pt'))\n",
    "cnn_model.eval()\n",
    "cnn_model.to(device)\n",
    "max_new_tokens = block_size-1\n",
    "image_path = '../All_Data/valid/Bluetick/2.jpg'\n",
    "\n",
    "valid_transformer = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "30d4a582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 6.5405e-09]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "this standing\n",
      "tensor([[1.0000e+00, 1.3605e-07]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "is this\n",
      "tensor([[9.9999e-01, 3.3799e-06]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "a collie\n",
      "tensor([[0.2498, 0.1648]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "standing yellow\n",
      "tensor([[0.6000, 0.0512]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "dog american\n",
      "tensor([[0.6869, 0.2957]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "hairless spaniel\n",
      "tensor([[9.9913e-01, 1.1803e-04]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      "dog standing\n",
      "tensor([[0.6545, 0.2404]], device='cuda:0', grad_fn=<TopkBackward0>)\n",
      ". on\n",
      "['this', 'is', 'a', 'standing', 'american', 'hairless', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "def generate(image_path, device, max_new_tokens, temperature=1.0, top_p=None, top_k=None):\n",
    "    if top_p is not None and top_k is not None:\n",
    "        print('Only one sampling approach is allowed')\n",
    "        return\n",
    "    if top_p is None and top_k is None:\n",
    "        print('You must select a sampling approach')\n",
    "        return\n",
    "    reverse_replace_dict = {'parrot':'Cockapoo', 'dalmatian':'Dalmation', 'bluebonnet':'Bluetick'}\n",
    "    generation_prob = []\n",
    "    generation_word = []\n",
    "    idx = None\n",
    "    image_file = Image.open(image_path)\n",
    "    image_tensor = valid_transformer(image_file).unsqueeze(0).to(device)\n",
    "    image_embedding = cnn_model(image_tensor)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(image_embedding, idx)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        if top_k is not None:\n",
    "            idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "            \n",
    "        if top_p is not None:\n",
    "            top_k = 1\n",
    "            idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "            while idx_next_prob.detach().cpu().sum().item() < top_p:\n",
    "                top_k += 1\n",
    "                idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "                \n",
    "        idx_next_prob, idx_next = idx_next_prob[0].detach().cpu(), idx_next[0].detach().cpu()\n",
    "        sum_prob = idx_next_prob.sum().item()\n",
    "        idx_next_prob = [prob_i/sum_prob for prob_i in idx_next_prob.tolist()]\n",
    "        idx_next_prob[-1] = 1 - sum(idx_next_prob[:-1]) \n",
    "        if idx_next_prob[-1] < 0:\n",
    "            idx_next_prob[-1] = 0\n",
    "            index = -1\n",
    "            while sum(idx_next_prob[:index]) > 1:\n",
    "                idx_next_prob[index-1] = 0\n",
    "                index -= 1\n",
    "            idx_next_prob [index] = 1 - sum(idx_next_prob[:index])\n",
    "\n",
    "        select_token = np.random.choice(idx_next.tolist(), 1, p=idx_next_prob)  \n",
    "        for token, token_prob in zip(idx_next, idx_next_prob):\n",
    "            if token == select_token[0]:\n",
    "                generation_prob.append(token_prob)    \n",
    "       \n",
    "        next_word = glove.itos[select_token[0]]\n",
    "\n",
    "        if next_word in reverse_replace_dict.keys():\n",
    "            next_word = reverse_replace_dict[next_word]\n",
    "            \n",
    "        generation_word.append(next_word)\n",
    "        test_prob, test_next = torch.topk(probs, k=2, dim=-1)\n",
    "        print(test_prob)\n",
    "        print(glove.itos[test_next[0, 0].detach().cpu().item()], glove.itos[test_next[0, 1].detach().cpu().item()])            \n",
    "            \n",
    "            \n",
    "        idx_next = torch.tensor(select_token).unsqueeze(0).to(device)\n",
    "        if idx is None:\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = idx_next\n",
    "        else:\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "        if generation_word[-1] == '.':\n",
    "            break\n",
    "\n",
    "    return generation_word, generation_prob\n",
    "\n",
    "generation_word, generation_prob = generate(image_path, device, max_new_tokens, temperature=1, top_p=None, top_k=3)\n",
    "print(generation_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46efecaf",
   "metadata": {},
   "source": [
    "# Compute Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "155b7339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'not', 'dog', '.'], ['he', 'is', 'a', 'good', 'dog', '.'], ['he', 'is', 'the', 'best', 'dog', '.']]\n",
      "BLEU score -> 1.0761741601727465e-31\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [\n",
    "#     'he is a not dog .'.split(),\n",
    "#     'he is a good dog .'.split(),\n",
    "#     'he is the best dog .'.split()\n",
    "]\n",
    "print(reference)\n",
    "print('BLEU score -> {}'.format(sentence_bleu(reference, generation_word, weights=(0.7,0.2,0.1))))\n",
    "#https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dea6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574dc33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
