{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f14cb8ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zixua/.cache\\torch\\hub\\saahiluppal_catr_master\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 140/140 [06:45<00:00,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average BLEU score is  0.816136045094537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "from models import caption\n",
    "from datasets import coco, utils\n",
    "from configuration import Config\n",
    "import os\n",
    "import warnings\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "# parser = argparse.ArgumentParser(description='Image Captioning')\n",
    "# parser.add_argument('--path', type=str, help='path to image', required=True)\n",
    "# parser.add_argument('--v', type=str, help='version', default='v3')\n",
    "# parser.add_argument('--checkpoint', type=str, help='checkpoint path', default=None)\n",
    "# args = parser.parse_args()\n",
    "# image_path = args.path\n",
    "# version = args.v\n",
    "# checkpoint_path = args.checkpoint\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "version = 'v3'\n",
    "\n",
    "config = Config()\n",
    "\n",
    "if version == 'v1':\n",
    "    model = torch.hub.load('saahiluppal/catr', 'v1', pretrained=True)\n",
    "elif version == 'v2':\n",
    "    model = torch.hub.load('saahiluppal/catr', 'v2', pretrained=True)\n",
    "elif version == 'v3':\n",
    "    model = torch.hub.load('saahiluppal/catr', 'v3', pretrained=True)\n",
    "    torch.save(model.state_dict(), './saved_checkpoints/pretrained_model.pt')\n",
    "    model.load_state_dict(torch.load('./saved_checkpoints/best_CPTR.pt'))\n",
    "else:\n",
    "    print(\"Checking for checkpoint.\")\n",
    "    if checkpoint_path is None:\n",
    "        raise NotImplementedError('No model to chose from!')\n",
    "    else:\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            raise NotImplementedError('Give valid checkpoint path')\n",
    "        print(\"Found checkpoint! Loading!\")\n",
    "        model,_ = caption.build_model(config)\n",
    "        print(\"Loading Checkpoint...\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "start_token = tokenizer.convert_tokens_to_ids(tokenizer._cls_token)\n",
    "end_token = tokenizer.convert_tokens_to_ids(tokenizer._sep_token)\n",
    "\n",
    "def create_caption_and_mask(start_token, max_length):\n",
    "    caption_template = torch.zeros((1, max_length), dtype=torch.long)\n",
    "    mask_template = torch.ones((1, max_length), dtype=torch.bool)\n",
    "\n",
    "    caption_template[:, 0] = start_token\n",
    "    mask_template[:, 0] = False\n",
    "\n",
    "    return caption_template, mask_template\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(image_path, devic, temperature=1.0, top_p=None, top_k=None):\n",
    "    if top_p is not None and top_k is not None:\n",
    "        print('Only one sampling approach is allowed')\n",
    "        return\n",
    "    if top_p is None and top_k is None:\n",
    "        print('You must select a sampling approach')\n",
    "        return\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = coco.val_transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    generation_prob = []\n",
    "    #caption.shape(1, max_length)\n",
    "    caption, cap_mask = create_caption_and_mask(\n",
    "        start_token, config.max_position_embeddings)    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    for i in range(config.max_position_embeddings - 1):\n",
    "        #caption, cap_mask = (1,128) 128 is max_length\n",
    "        #prediction.shape = torch.Size([1, 128, 30522])\n",
    "        #image.shape = (1,3,299,299)\n",
    "        predictions = model(image.to(device), caption.to(device), cap_mask.to(device))\n",
    "        predictions = predictions[:, i, :] / temperature\n",
    "        probs = F.softmax(predictions, dim=-1)\n",
    "        if top_k is not None:\n",
    "            idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "            \n",
    "        if top_p is not None:\n",
    "            top_k = 1\n",
    "            idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "            while idx_next_prob.detach().cpu().sum().item() < top_p:\n",
    "                top_k += 1\n",
    "                idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "        \n",
    "        idx_next_prob, idx_next = idx_next_prob[0].detach().cpu(), idx_next[0].detach().cpu()\n",
    "        sum_prob = idx_next_prob.sum().item()\n",
    "        idx_next_prob = [prob_i/sum_prob for prob_i in idx_next_prob.tolist()]\n",
    "        idx_next_prob[-1] = 1 - sum(idx_next_prob[:-1]) \n",
    "        if idx_next_prob[-1] < 0:\n",
    "            idx_next_prob[-1] = 0\n",
    "            index = -1\n",
    "            while sum(idx_next_prob[:index]) > 1:\n",
    "                idx_next_prob[index-1] = 0\n",
    "                index -= 1\n",
    "            idx_next_prob [index] = 1 - sum(idx_next_prob[:index])\n",
    "\n",
    "        select_token = np.random.choice(idx_next.tolist(), 1, p=idx_next_prob)  \n",
    "        for token, token_prob in zip(idx_next, idx_next_prob):\n",
    "            if token == select_token[0]:\n",
    "                generation_prob.append(token_prob)    \n",
    "                         \n",
    "        predicted_id = torch.argmax(predictions, axis=-1)\n",
    "        if select_token[0] == 102 or select_token[0] == 1012:\n",
    "            return caption, generation_prob\n",
    "\n",
    "        caption[:, i+1] = select_token[0]\n",
    "        cap_mask[:, i+1] = False\n",
    "        \n",
    "    return caption, generation_prob\n",
    "\n",
    "def generate_n_captions(image_path, num_examples, reference):\n",
    "#     image = Image.open(image_path)\n",
    "#     print('The input image is:')\n",
    "#     plt.imshow(image, vmin=0, vmax=255)\n",
    "#     plt.show()\n",
    "    all_generation_word = []\n",
    "    all_generation_prob = []\n",
    "    reverse_replace_dict = {'parrot':'Cockapoo', 'dalmatian':'Dalmation', 'bluebonnet':'Bluetick', 'pere':'Perenees', 'goren':'Groenendael', 'shih':'Shih-Tzu', 'shar':'Shar_Pei', 'komon':'Komondor'}\n",
    "#     print('The generated captions are:')\n",
    "    for _ in range(num_examples):\n",
    "        caption, generation_prob = predict(image_path, device, temperature=1.0, top_p=None, top_k=3)\n",
    "        result = tokenizer.decode(caption[0].tolist(), skip_special_tokens=True)\n",
    "        sting_list = result.split()\n",
    "        combined_string = ''\n",
    "        for i, word in enumerate (sting_list):\n",
    "            if word in reverse_replace_dict.keys():\n",
    "                sting_list[i] = reverse_replace_dict[word]\n",
    "            if i != 0:\n",
    "                combined_string += ' '\n",
    "            combined_string += sting_list[i]    \n",
    "\n",
    "        #result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#         print(combined_string.capitalize())\n",
    "        all_generation_word.append(sting_list)\n",
    "        all_generation_prob.append(generation_prob)\n",
    "\n",
    "    BLEU_score = []\n",
    "    for generation_word in all_generation_word:\n",
    "        BLEU_score.append(sentence_bleu(reference, generation_word, weights=[1]))\n",
    "\n",
    "    return sum(BLEU_score)/len(BLEU_score)\n",
    "\n",
    "\n",
    "\n",
    "csv_path_valid = '../All_Data/annotations_valid.csv'\n",
    "image_path_valid = '../All_Data/valid'\n",
    "num_examples = 4\n",
    "annotations = pd.read_csv(csv_path_valid)\n",
    "captions = np.array(annotations['captions'])\n",
    "image_names = np.array(annotations['file_directory'])   \n",
    "avg_BLEU_all_images = []\n",
    "\n",
    "for i, cap in enumerate(captions):\n",
    "    captions[i] = captions[i][:-2]\n",
    "\n",
    "with tqdm.tqdm(total=int(len(captions)/4)) as pbar:\n",
    "    for start in range(int(len(captions)/4)):\n",
    "        index_start = start*num_examples\n",
    "        index_end = (start+1)*num_examples\n",
    "        image_path = image_names[index_start]\n",
    "        image_path = '../All_Data/valid/' + image_path\n",
    "        reference = captions[index_start:index_end].tolist()\n",
    "        reference_split = []\n",
    "\n",
    "        for ref in reference:\n",
    "            reference_split.append(ref.split())\n",
    "\n",
    "        avg_BLEU_one_image = generate_n_captions(image_path, num_examples, reference_split)\n",
    "        avg_BLEU_all_images.append(avg_BLEU_one_image)\n",
    "        pbar.update(1)\n",
    "\n",
    "print('The average BLEU score is ',sum(avg_BLEU_all_images)/len(avg_BLEU_all_images))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25440f76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
