{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f91fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# csv_path = 'test.csv'\n",
    "\n",
    "# dictionary = {'a':['123','234','345'], 'b':['567','678','789']}\n",
    "# df = pd.DataFrame.from_dict(dictionary, orient='columns')\n",
    "# print(df)\n",
    "# df.to_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca7ed2d",
   "metadata": {},
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf01e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext\n",
    "import math\n",
    "from PIL import Image\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from mingpt.utils import CfgNode as CN\n",
    "import time\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac7cf1",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31db5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_train = '../All_Data/annotations_train.csv'\n",
    "image_path_train = '../All_Data/train'\n",
    "csv_path_valid = '../All_Data/annotations_valid.csv'\n",
    "image_path_valid = '../All_Data/valid'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "n_embed = 100\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\",dim=n_embed) # embedding size = 100  \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f09c2",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc46f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, image_folder_path, vocab, csv_path, custom_transforms):\n",
    "        self.image_folder_path = image_folder_path\n",
    "        annotations = pd.read_csv(csv_path)\n",
    "        self.captions = np.array(annotations['captions'])\n",
    "        self.image_names = np.array(annotations['file_directory'])    \n",
    "        self.custom_transforms = custom_transforms\n",
    "        self.replace_dict = {'Cockapoo':'parrot', 'Dalmation':'dalmatian', 'Bluetick':'bluebonnet', 'Perenees':'pere', 'Groenendael':'goren', 'Shih-Tzu':'shih', 'Shar_Pei':'shar', 'Komondor':'komon'}\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_full_name = self.image_folder_path + '/' + self.image_names[idx]\n",
    "        x = Image.open(image_full_name)\n",
    "        if self.custom_transforms is not None:\n",
    "            x = self.custom_transforms(x)\n",
    "#             x = x.permute(1,2,0)\n",
    "        ground_truth_cap = self.captions[idx]\n",
    "        V = len(self.vocab.vectors)\n",
    "        L = ground_truth_cap.split()\n",
    "        for i, word in enumerate (L):\n",
    "            if word in self.replace_dict.keys():\n",
    "                L[i] = self.replace_dict[word]\n",
    "                \n",
    "        tokenized_caption = torch.tensor([self.vocab.stoi.get(w.lower(), V-1) for w in L])  # Use the last word in the vocab as the \"out-of-vocabulary\" token\n",
    "        if V-1 in tokenized_caption:\n",
    "            print('Wrong Labelling')\n",
    "#         print(image_full_name, ground_truth_cap)        \n",
    "        return x, tokenized_caption\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab.vectors)  \n",
    "    \n",
    "    def get_block_size(self):\n",
    "        all_captions_len = []\n",
    "        for i in range(len(self.captions)):\n",
    "            all_captions_len.append(len(self.captions[i].split()))\n",
    "        return max(all_captions_len)+1\n",
    "    \n",
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.3),\n",
    "    torchvision.transforms.RandomRotation(degrees=10),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "valid_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "def lm_collate_fn(batch, device):\n",
    "    x = [item[0] for item in batch]  \n",
    "    y_input = [item[1][:-1] for item in batch]  \n",
    "    y_label = [item[1] for item in batch]\n",
    "    maxlen_input = max([len(s) for s in y_input])\n",
    "    maxlen_label = max([len(s) for s in y_label])\n",
    "    padding_value = glove.stoi.get('unk')\n",
    "    #x from first word to the second last word, y from second word to the last word\n",
    "    input_cap, label_cap = [], []\n",
    "    for sy_i, sy_l in zip(y_input, y_label):\n",
    "        input_cap.append(torch.cat([sy_i, torch.ones(maxlen_input - len(sy_i))*padding_value]))\n",
    "        label_cap.append(torch.cat([sy_l, torch.ones(maxlen_label - len(sy_l))*padding_value]))\n",
    "    return torch.stack(x).long().to(device), torch.stack(input_cap).long().to(device), torch.stack(label_cap).long().to(device)\n",
    "  \n",
    "train_dataset = Custom_Dataset(image_path_train, glove, csv_path_train, train_transforms)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn = lambda batch: lm_collate_fn(batch, device))\n",
    "\n",
    "valid_dataset = Custom_Dataset(image_path_valid, glove, csv_path_valid, valid_transforms)\n",
    "batch_size = 32\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn = lambda batch: lm_collate_fn(batch, device))\n",
    "\n",
    "vocab_size = train_dataset.get_vocab_size()\n",
    "block_size = train_dataset.get_block_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b624598",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "018500df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.py\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class NewGELU(nn.Module):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x.shape = (batch, sent len, embedding)\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\" GPT Language Model \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        # either model_type or (n_layer, n_head, n_embd) must be given in the config\n",
    "        C.model_type = 'gpt'\n",
    "        C.n_layer = None\n",
    "        C.n_head = None\n",
    "        C.n_embd =  None\n",
    "        # these options must be filled in externally\n",
    "        C.vocab_size = None\n",
    "        C.block_size = None\n",
    "        # dropout hyperparameters\n",
    "        C.embd_pdrop = 0.1\n",
    "        C.resid_pdrop = 0.1\n",
    "        C.attn_pdrop = 0.1\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, vocab):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        type_given = config.model_type is not None\n",
    "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
    "        assert type_given ^ params_given # exactly one of these (XOR)\n",
    "        if type_given:\n",
    "            # translate from model_type to detailed configuration\n",
    "            config.merge_from_dict({\n",
    "                # names follow the huggingface naming conventions\n",
    "                # GPT-1\n",
    "                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n",
    "                # GPT-2 configs\n",
    "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "                # Gophers\n",
    "                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
    "                # (there are a number more...)\n",
    "                # I made these tiny models up\n",
    "                'gpt-mini':     dict(n_layer=5, n_head=5, n_embd=n_embed),\n",
    "                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
    "                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
    "            }[config.model_type])\n",
    "\n",
    "        #wte is embedding for words\n",
    "        #wpe is embedding for positions\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding.from_pretrained(vocab.vectors, freeze = True),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def configure_optimizers(self, train_config, cnn_model_params):\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "                # random note: because named_modules and named_parameters are recursive\n",
    "                # we will see the same tensors p many many times. but doing it this way\n",
    "                # allows us to know which parent module any tensor p belongs to...\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "        if cnn_model_params is not None:\n",
    "            # create the pytorch optimizer object\n",
    "            optim_groups = [\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "                {\"params\": cnn_model_params,'lr': 3e-5}\n",
    "            ]\n",
    "            \n",
    "            n_parameters_transformer = sum(p.numel()\n",
    "               for p in self.parameters() if p.requires_grad)\n",
    "            n_parameters_cnn = sum(p.numel()\n",
    "                           for p in cnn_model_params if p.requires_grad)\n",
    "            print(f\"Number of trainable params: {n_parameters_transformer + n_parameters_cnn}\")\n",
    "        else:\n",
    "            # create the pytorch optimizer object\n",
    "            optim_groups = [\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "                {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0}\n",
    "            ]  \n",
    "        \n",
    "            n_parameters_transformer = sum(p.numel()\n",
    "                           for p in self.parameters() if p.requires_grad)\n",
    "            print(f\"Number of trainable params: {n_parameters_transformer}\")\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, image_embed, idx=None, targets=None, finetune_classify=False):\n",
    "\n",
    "        device = image_embed.device\n",
    "  \n",
    "        if idx is not None:\n",
    "            b, t = idx.size()\n",
    "            assert t <= self.block_size, f\"Cannot forwarnd sequence of length {t}, block size is only {self.block_size}\"\n",
    "            pos = torch.arange(0, t+1, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "            # forward the GPT model itself\n",
    "            tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            tok_emb = torch.cat((image_embed.unsqueeze(1), tok_emb), 1)\n",
    "          \n",
    "        else:\n",
    "            pos = torch.arange(0, 1, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "            tok_emb = image_embed.unsqueeze(1)   \n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        \n",
    "        assert tok_emb[0].shape == pos_emb[0].shape, f\"wrong token or position embedding\"\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        #x.shape = (batch, sentence len, embedding)\n",
    "        if not finetune_classify:\n",
    "            # LM forward procedure\n",
    "            logits = self.lm_head(x)\n",
    "        else:\n",
    "            # Finetune classify procedure\n",
    "            print('error')\n",
    "            return\n",
    "            \n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbf39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 40.61M\n"
     ]
    }
   ],
   "source": [
    "# set up model configurations\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-mini'\n",
    "model_config.vocab_size = vocab_size\n",
    "#block_size is a max sentence length in dataset\n",
    "model_config.block_size = block_size\n",
    "model = GPT(model_config, glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50483b61",
   "metadata": {},
   "source": [
    "# Load Model and Generate Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df6085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zixua/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "cnn_model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True,)\n",
    "cnn_model.classifier = nn.Sequential(nn.Linear(1280, 512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Dropout(0.2),\n",
    "                                    nn.Linear(512, n_embed))\n",
    "\n",
    "model.load_state_dict(torch.load('./saved_checkpoints/best_transformer.pt'))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "cnn_model.load_state_dict(torch.load('./saved_checkpoints/best_cnn.pt'))\n",
    "cnn_model.eval()\n",
    "cnn_model.to(device)\n",
    "max_new_tokens = block_size-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d4a582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 140/140 [00:43<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average BLEU score is  0.5716106502843731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def generate(image_path, device, max_new_tokens, temperature=1.0, top_p=None, top_k=None):\n",
    "    if top_p is not None and top_k is not None:\n",
    "        print('Only one sampling approach is allowed')\n",
    "        return\n",
    "    if top_p is None and top_k is None:\n",
    "        print('You must select a sampling approach')\n",
    "        return\n",
    "    reverse_replace_dict = {'parrot':'Cockapoo', 'dalmatian':'Dalmation', 'bluebonnet':'Bluetick', 'pere':'Perenees', 'goren':'Groenendael', 'shih':'Shih-Tzu', 'shar':'Shar_Pei', 'komon':'Komondor'}\n",
    "    generation_prob = []\n",
    "    generation_word = []\n",
    "    idx = None\n",
    "    image_file = Image.open(image_path)\n",
    "    image_tensor = valid_transforms(image_file).unsqueeze(0).to(device)\n",
    "    image_embedding = cnn_model(image_tensor)\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, _ = model(image_embedding, idx)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        if top_k is not None:\n",
    "            idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "            \n",
    "        if top_p is not None:\n",
    "            top_k = 1\n",
    "            idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "            while idx_next_prob.detach().cpu().sum().item() < top_p:\n",
    "                top_k += 1\n",
    "                idx_next_prob, idx_next = torch.topk(probs, k=top_k, dim=-1)\n",
    "                \n",
    "        idx_next_prob, idx_next = idx_next_prob[0].detach().cpu(), idx_next[0].detach().cpu()\n",
    "        sum_prob = idx_next_prob.sum().item()\n",
    "        idx_next_prob = [prob_i/sum_prob for prob_i in idx_next_prob.tolist()]\n",
    "        idx_next_prob[-1] = 1 - sum(idx_next_prob[:-1]) \n",
    "        if idx_next_prob[-1] < 0:\n",
    "            idx_next_prob[-1] = 0\n",
    "            index = -1\n",
    "            while sum(idx_next_prob[:index]) > 1:\n",
    "                idx_next_prob[index-1] = 0\n",
    "                index -= 1\n",
    "            idx_next_prob [index] = 1 - sum(idx_next_prob[:index])\n",
    "\n",
    "        select_token = np.random.choice(idx_next.tolist(), 1, p=idx_next_prob)  \n",
    "        for token, token_prob in zip(idx_next, idx_next_prob):\n",
    "            if token == select_token[0]:\n",
    "                generation_prob.append(token_prob)    \n",
    "       \n",
    "        next_word = glove.itos[select_token[0]]\n",
    "\n",
    "        if next_word in reverse_replace_dict.keys():\n",
    "            next_word = reverse_replace_dict[next_word]\n",
    "            \n",
    "        if next_word == '.':\n",
    "            break        \n",
    "            \n",
    "        generation_word.append(next_word)\n",
    "#         test_prob, test_next = torch.topk(probs, k=2, dim=-1)\n",
    "#         print(test_prob)\n",
    "#         print(glove.itos[test_next[0, 0].detach().cpu().item()], glove.itos[test_next[0, 1].detach().cpu().item()])            \n",
    "            \n",
    "            \n",
    "        idx_next = torch.tensor(select_token).unsqueeze(0).to(device)\n",
    "        if idx is None:\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = idx_next\n",
    "        else:\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        \n",
    "\n",
    "    return generation_word, generation_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_n_captions(image_path, num_examples, reference):\n",
    "#     image = Image.open(image_path)\n",
    "#     print('The input image is:')\n",
    "#     plt.imshow(image, vmin=0, vmax=255)\n",
    "#     plt.show()\n",
    "    all_generation_word = []\n",
    "    all_generation_prob = []\n",
    "#     print('The generated captions are:')\n",
    "    for _ in range(num_examples):\n",
    "        generation_word, generation_prob = generate(image_path, device, max_new_tokens, temperature=1, top_p=None, top_k=3)\n",
    "        all_generation_word.append(generation_word)\n",
    "        all_generation_prob.append(generation_prob)\n",
    "        combined_string = ''\n",
    "        for i, word in enumerate (generation_word):\n",
    "            if i != 0:\n",
    "                combined_string += ' '\n",
    "            combined_string += generation_word[i]    \n",
    "\n",
    "        #result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#         print(combined_string.capitalize())\n",
    "\n",
    "    BLEU_score = []\n",
    "    for generation_word in all_generation_word:\n",
    "        BLEU_score.append(sentence_bleu(reference, generation_word, weights=[1]))\n",
    "    \n",
    "    avg_BLEU_one_image = sum(BLEU_score)/len(BLEU_score)\n",
    "    return avg_BLEU_one_image\n",
    "\n",
    "        \n",
    "csv_path_valid = '../All_Data/annotations_valid.csv'\n",
    "image_path_valid = '../All_Data/valid'\n",
    "num_examples = 4\n",
    "annotations = pd.read_csv(csv_path_valid)\n",
    "captions = np.array(annotations['captions'])\n",
    "image_names = np.array(annotations['file_directory'])   \n",
    "avg_BLEU_all_images = []\n",
    "\n",
    "for i, cap in enumerate(captions):\n",
    "    captions[i] = captions[i][:-2]\n",
    "\n",
    "with tqdm.tqdm(total=int(len(captions)/4)) as pbar:\n",
    "    for start in range(int(len(captions)/4)):\n",
    "        index_start = start*num_examples\n",
    "        index_end = (start+1)*num_examples\n",
    "        image_path = image_names[index_start]\n",
    "        image_path = '../All_Data/valid/' + image_path\n",
    "        reference = captions[index_start:index_end].tolist()\n",
    "        reference_split = []\n",
    "\n",
    "        for ref in reference:\n",
    "            reference_split.append(ref.split())\n",
    "\n",
    "        avg_BLEU_one_image = generate_n_captions(image_path, num_examples, reference_split)\n",
    "        avg_BLEU_all_images.append(avg_BLEU_one_image)\n",
    "        pbar.update(1)\n",
    "\n",
    "print('The average BLEU score is ',sum(avg_BLEU_all_images)/len(avg_BLEU_all_images))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dea6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574dc33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8670314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
