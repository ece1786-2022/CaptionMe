{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9827bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "from models import utils\n",
    "# from datasets import coco\n",
    "from configuration import Config\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "import random\n",
    "from transformers import BertTokenizer\n",
    "from datasets.utils import *\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "# from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafc7df",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8831a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DIM = 299\n",
    "\n",
    "def under_max(image):\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "    shape = np.array(image.size, dtype=np.float)\n",
    "    long_dim = max(shape)\n",
    "    scale = MAX_DIM / long_dim\n",
    "\n",
    "    new_shape = (shape * scale).astype(int)\n",
    "    image = image.resize(new_shape)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "class RandomRotation:\n",
    "    def __init__(self, angles=[0, 90, 180, 270]):\n",
    "        self.angles = angles\n",
    "\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        return TF.rotate(x, angle, expand=True)\n",
    "\n",
    "\n",
    "train_transform = tv.transforms.Compose([\n",
    "    RandomRotation(),\n",
    "    tv.transforms.Lambda(under_max),\n",
    "    tv.transforms.ColorJitter(brightness=[0.5, 1.3], contrast=[\n",
    "                              0.8, 1.5], saturation=[0.2, 1.5]),\n",
    "    tv.transforms.RandomHorizontalFlip(),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "val_transform = tv.transforms.Compose([\n",
    "    tv.transforms.Lambda(under_max),\n",
    "    tv.transforms.ToTensor(),\n",
    "    tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "class CocoCaption(Dataset):\n",
    "    def __init__(self, root, ann, max_length, limit, transform=train_transform, mode='training'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.annot = [(val['image_id'], val['caption'])\n",
    "                      for val in ann['annotations']]\n",
    "        if mode == 'validation':\n",
    "            self.annot = self.annot\n",
    "        if mode == 'training':\n",
    "            self.annot = self.annot[: limit]\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "            'bert-base-uncased', do_lower=True)\n",
    "        self.max_length = max_length + 1\n",
    "\n",
    "    def _process(self, image_id):\n",
    "        val = str(image_id).zfill(12)\n",
    "        return val + '.jpg'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annot)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, caption = self.annot[idx]\n",
    "        image = Image.open(self.root + '/' +image_id)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        image = nested_tensor_from_tensor_list_data(image.unsqueeze(0))\n",
    "\n",
    "        caption_encoded = self.tokenizer.encode_plus(\n",
    "            caption, max_length=self.max_length, pad_to_max_length=True, return_attention_mask=True, return_token_type_ids=False, truncation=True)\n",
    "\n",
    "        caption = np.array(caption_encoded['input_ids'])\n",
    "        cap_mask = (\n",
    "            1 - np.array(caption_encoded['attention_mask'])).astype(bool)\n",
    "\n",
    "        return image.tensors.squeeze(0), image.mask.squeeze(0), caption, cap_mask\n",
    "\n",
    "\n",
    "def build_dataset(config, mode='training'):\n",
    "    if mode == 'training':\n",
    "        train_dir = config.dir+'/train'\n",
    "        train_file = config.dir+'/annotations_train.json'\n",
    "        data = CocoCaption(train_dir, read_json(\n",
    "            train_file), max_length=config.max_position_embeddings, limit=config.limit, transform=train_transform, mode='training')\n",
    "        return data\n",
    "\n",
    "    elif mode == 'validation':\n",
    "        val_dir = config.dir+'/valid'\n",
    "        val_file = config.dir+'/annotations_valid.json'\n",
    "        data = CocoCaption(val_dir, read_json(\n",
    "            val_file), max_length=config.max_position_embeddings, limit=config.limit, transform=val_transform, mode='validation')\n",
    "        return data\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{mode} not supported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023ebfd",
   "metadata": {},
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6a2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import utils\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "def train_one_epoch(model, criterion, data_loader,\n",
    "                    optimizer, device, epoch, max_norm):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    total = len(data_loader)\n",
    "\n",
    "    with tqdm.tqdm(total=total) as pbar:\n",
    "        for images, masks, caps, cap_masks in data_loader:\n",
    "            samples = utils.NestedTensor(images, masks).to(device)\n",
    "            caps = caps.to(device)\n",
    "            cap_masks = cap_masks.to(device)\n",
    "\n",
    "            outputs = model(samples, caps[:, :-1], cap_masks[:, :-1])\n",
    "            loss = criterion(outputs.permute(0, 2, 1), caps[:, 1:].type(torch.LongTensor).to(device))\n",
    "            loss_value = loss.item()\n",
    "            epoch_loss += loss_value\n",
    "\n",
    "            if not math.isfinite(loss_value):\n",
    "                print(f'Loss is {loss_value}, stopping training')\n",
    "                sys.exit(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if max_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "            pbar.update(1)\n",
    "\n",
    "    return epoch_loss / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, criterion, data_loader, device):\n",
    "    model.eval()\n",
    "    criterion.eval()\n",
    "\n",
    "    validation_loss = 0.0\n",
    "    total = len(data_loader)\n",
    "\n",
    "    with tqdm.tqdm(total=total) as pbar:\n",
    "        for images, masks, caps, cap_masks in data_loader:\n",
    "            samples = utils.NestedTensor(images, masks).to(device)\n",
    "            caps = caps.to(device)\n",
    "            cap_masks = cap_masks.to(device)\n",
    "\n",
    "            outputs = model(samples, caps[:, :-1], cap_masks[:, :-1])\n",
    "            loss = criterion(outputs.permute(0, 2, 1), caps[:, 1:])\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            pbar.update(1)\n",
    "        \n",
    "    return validation_loss / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73791e46",
   "metadata": {},
   "source": [
    "# Backbone Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4506fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "#https://blog.csdn.net/qq_35037684/article/details/118975018\n",
    "from typing import Dict, List\n",
    "\n",
    "from models.utils import NestedTensor, is_main_process\n",
    "\n",
    "from models.position_encoding import build_position_encoding\n",
    "\n",
    "\n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
    "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
    "    produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = 1e-5\n",
    "        scale = w * (rv + eps).rsqrt()\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "\n",
    "class BackboneBase(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, train_backbone: bool, num_channels: int, return_interm_layers: bool):\n",
    "        super().__init__()\n",
    "        for name, parameter in backbone.named_parameters():\n",
    "            if not train_backbone or 'layer2' not in name and 'layer3' not in name and 'layer4' not in name:\n",
    "                parameter.requires_grad_(False)\n",
    "        if return_interm_layers:\n",
    "            return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
    "        else:\n",
    "            return_layers = {'layer4': \"0\"}\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        xs = self.body(tensor_list.tensors)\n",
    "        out: Dict[str, NestedTensor] = {}\n",
    "        for name, x in xs.items():\n",
    "            #x.shape = (B, 2048, 19, 19)\n",
    "            m = tensor_list.mask\n",
    "            assert m is not None\n",
    "            mask = F.interpolate(m[None].float(), size=x.shape[-2:]).to(torch.bool)[0]\n",
    "            #mask.shape = 19, 19\n",
    "            out[name] = NestedTensor(x, mask)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Backbone(BackboneBase):\n",
    "    \"\"\"ResNet backbone with frozen BatchNorm.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_backbone: bool,\n",
    "                 return_interm_layers: bool,\n",
    "                 dilation: bool):\n",
    "        backbone = getattr(torchvision.models, name)(\n",
    "            replace_stride_with_dilation=[False, False, dilation],\n",
    "            pretrained=is_main_process(), norm_layer=FrozenBatchNorm2d)\n",
    "        num_channels = 512 if name in ('resnet18', 'resnet34') else 2048\n",
    "        super().__init__(backbone, train_backbone, num_channels, return_interm_layers)\n",
    "\n",
    "\n",
    "class Joiner(nn.Sequential):\n",
    "    def __init__(self, backbone, position_embedding):\n",
    "        super().__init__(backbone, position_embedding)\n",
    "\n",
    "    def forward(self, tensor_list: NestedTensor):\n",
    "        xs = self[0](tensor_list)\n",
    "        out: List[NestedTensor] = []\n",
    "        pos = []\n",
    "        for name, x in xs.items():\n",
    "            out.append(x)\n",
    "            # position encoding\n",
    "            pos.append(self[1](x).to(x.tensors.dtype))\n",
    "        #pos.shape = list with one element of (B, 256, 19, 19)\n",
    "        return out, pos\n",
    "\n",
    "\n",
    "def build_backbone(config):\n",
    "    position_embedding = build_position_encoding(config)\n",
    "    train_backbone = config.lr_backbone > 0\n",
    "    return_interm_layers = False\n",
    "    backbone = Backbone(config.backbone, train_backbone, return_interm_layers, config.dilation)\n",
    "    model = Joiner(backbone, position_embedding)\n",
    "    model.num_channels = backbone.num_channels\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72826d",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c49269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.utils import NestedTensor, nested_tensor_from_tensor_list\n",
    "from models.transformer import build_transformer\n",
    "\n",
    "\n",
    "class Caption(nn.Module):\n",
    "    def __init__(self, backbone, transformer, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.input_proj = nn.Conv2d(\n",
    "            backbone.num_channels, hidden_dim, kernel_size=1)\n",
    "        self.transformer = transformer\n",
    "        self.mlp = MLP(hidden_dim, 512, vocab_size, 3)\n",
    "\n",
    "    def forward(self, samples, target, target_mask):\n",
    "        if not isinstance(samples, NestedTensor):\n",
    "            samples = nested_tensor_from_tensor_list(samples)\n",
    "\n",
    "        features, pos = self.backbone(samples)\n",
    "        src, mask = features[-1].decompose()\n",
    "        assert mask is not None\n",
    "\n",
    "        hs = self.transformer(self.input_proj(src), mask,\n",
    "                              pos[-1], target, target_mask)\n",
    "        out = self.mlp(hs.permute(1, 0, 2))\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(nn.Linear(n, k)\n",
    "                                    for n, k in zip([input_dim] + h, h + [output_dim]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    backbone = build_backbone(config)\n",
    "    transformer = build_transformer(config)\n",
    "\n",
    "    model = Caption(backbone, transformer, config.hidden_dim, config.vocab_size)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    return model, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952132b7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Device: cuda\n",
      "Number of trainable params: 83959866\n",
      "Train: 1883\n",
      "Valid: 1884\n",
      "Start Training 20 epochs\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 117/117 [02:11<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.13209020629779905\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 117/117 [01:57<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.05240556196524547\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 117/117 [01:57<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.03493226177863076\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                                | 2/117 [00:03<02:54,  1.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 73>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     74\u001b[0m     config \u001b[38;5;241m=\u001b[39m Config()\n\u001b[1;32m---> 75\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mstart_epoch, config\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_max_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, criterion, data_loader, optimizer, device, epoch, max_norm)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m max_norm \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     29\u001b[0m             torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm)\n\u001b[1;32m---> 30\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss \u001b[38;5;241m/\u001b[39m total\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\optim\\adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    159\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 161\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\optim\\adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 218\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test0\\lib\\site-packages\\torch\\optim\\adamw.py:266\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    263\u001b[0m param\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(config):\n",
    "    print(f'Initializing Device: {device}')\n",
    "\n",
    "    seed = config.seed + utils.get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model, criterion = build_model(config)\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    model.to(device)\n",
    "    n_parameters = sum(p.numel()\n",
    "                       for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable params: {n_parameters}\")\n",
    "\n",
    "    param_dicts = [\n",
    "        {\"params\": [p for n, p in model.named_parameters(\n",
    "        ) if \"backbone\" not in n and p.requires_grad]},\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "            \"lr\": config.lr_backbone,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        param_dicts, lr=config.lr, weight_decay=config.weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, config.lr_drop)\n",
    "\n",
    "    dataset_train = build_dataset(config, mode='training')\n",
    "    dataset_val = build_dataset(config, mode='validation')\n",
    "    print(f\"Train: {len(dataset_train)}\")\n",
    "    print(f\"Valid: {len(dataset_val)}\")\n",
    "\n",
    "    sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "\n",
    "    batch_sampler_train = torch.utils.data.BatchSampler(\n",
    "        sampler_train, config.batch_size, drop_last=True\n",
    "    )\n",
    "\n",
    "    data_loader_train = DataLoader(\n",
    "        dataset_train, batch_sampler=batch_sampler_train)\n",
    "    data_loader_val = DataLoader(dataset_val, config.batch_size,\n",
    "                                 sampler=sampler_val, drop_last=False)    \n",
    "    \n",
    "    if os.path.exists(config.checkpoint):\n",
    "        print(\"Loading Checkpoint...\")\n",
    "        checkpoint = torch.load(config.checkpoint, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        config.start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    print(\"Start Training {} epochs\".format(config.epochs))\n",
    "    for epoch in range(config.start_epoch, config.epochs):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        epoch_loss = train_one_epoch(\n",
    "            model, criterion, data_loader_train, optimizer, device, epoch, config.clip_max_norm)\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Training Loss: {epoch_loss}\")\n",
    "        torch.save(model.state_dict(), 'best_model_fine_tunned.pt')\n",
    "#         torch.save({\n",
    "#             'model': model.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict(),\n",
    "#             'lr_scheduler': lr_scheduler.state_dict(),\n",
    "#             'epoch': epoch,\n",
    "#         }, config.checkpoint)\n",
    "\n",
    "#         validation_loss = evaluate(model, criterion, data_loader_val, device)\n",
    "#         print(f\"Validation Loss: {validation_loss}\")\n",
    "\n",
    "#         print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    main(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f328d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa731faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb1c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
